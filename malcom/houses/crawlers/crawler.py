import logging
import re
from abc import ABC
from collections.abc import Callable
from datetime import date, datetime
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from django.core.exceptions import ValidationError
from django.db import transaction
from django.utils import timezone
from performers.models import Performer, PerformerSocialLink
from pykakasi import kakasi

from ..definitions import CrawlerCollectionState, WebsiteProcessingState
from ..models import LiveHouse, LiveHouseWebsite, PerformanceSchedule, PerformanceScheduleTicketPurchaseInfo

logger = logging.getLogger(__name__)

# Constants for validation and limits
MIN_LIVEHOUSE_CAPACITY = 50  # noqa: N806
MAX_LIVEHOUSE_CAPACITY = 350  # noqa: N806
MIN_TICKET_PRICE = 500  # noqa: N806
MAX_TICKET_PRICE = 20000  # noqa: N806
MIN_YEAR = 2024  # noqa: N806
MAX_YEAR = 2026  # noqa: N806
MONTHS_PER_YEAR = 12  # noqa: N806
MAX_DAYS_PER_MONTH = 31  # noqa: N806
MAX_SCHEDULES_PER_FETCH = 20  # noqa: N806
MAX_PERFORMERS_TO_DISPLAY = 5  # noqa: N806
MIN_PERFORMER_NAME_LENGTH = 2  # noqa: N806
MIN_SLASH_PARTS = 2  # noqa: N806
HTTP_SUCCESS = 200  # noqa: N806
MAX_SOCIAL_LINKS = 5  # noqa: N806
MAX_CONTEXT_CHARS = 200  # noqa: N806
MAX_PERFORMERS_IN_CONTEXT = 3  # noqa: N806


class PerformerValidationError(ValueError):
    """Raised when a performer fails validation as a legitimate artist."""


class CrawlerRegistry:
    """Registry to manage crawler classes."""

    _crawlers = {}

    @classmethod
    def register(cls, crawler_name: str) -> Callable[[type], type]:
        """Decorator to register a crawler class."""

        def decorator(crawler_class: type) -> type:
            cls._crawlers[crawler_name] = crawler_class
            return crawler_class

        return decorator

    @classmethod
    def get_crawler(cls, crawler_name: str) -> type | None:
        """Get a crawler class by name."""
        return cls._crawlers.get(crawler_name)

    @classmethod
    def run_crawler(cls, website: LiveHouseWebsite) -> None:
        """Run the appropriate crawler for a website."""
        crawler_class = cls.get_crawler(website.crawler_class)
        if crawler_class:
            crawler = crawler_class(website)
            crawler.run()
        else:
            msg = f"No crawler found for class: {website.crawler_class}"
            raise ValueError(msg)  # noqa: B904


@CrawlerRegistry.register("LiveHouseWebsiteCrawler")
class LiveHouseWebsiteCrawler(ABC):  # noqa: B024
    """Base class for live house website crawlers."""

    def __init__(self, website: LiveHouseWebsite) -> None:
        self.website = website
        self.base_url = website.url
        self.session = requests.Session()
        # Set timeout for all requests
        self.timeout = 30  # 30 seconds timeout

    def run(self) -> None:
        """Main method to run the crawler for a specific website."""
        logger.info(f"Starting crawler for: {self.website.url}")

        with transaction.atomic():
            # Update state to in_progress
            self.website.state = WebsiteProcessingState.IN_PROGRESS
            self.website.save()

            try:
                # Fetch main page
                logger.debug(f"Fetching main page: {self.website.url}")
                main_page_content = self.fetch_page(self.website.url)
                logger.debug(f"Fetched {len(main_page_content)} characters from main page")

                # Extract live house information
                logger.debug("Extracting live house info")
                live_house_data = self.extract_live_house_info(main_page_content)
                logger.debug(f"Extracted live house data: {live_house_data}")

                # Create or update LiveHouse instance
                live_house = self.create_or_update_live_house(live_house_data)
                logger.info(f"Created/updated LiveHouse: {live_house.name}")

                # Find schedule page link
                logger.debug("Finding schedule page link")
                schedule_url = self.find_schedule_link(main_page_content)
                logger.debug(f"Schedule URL found: {schedule_url}")

                if schedule_url:
                    # Fetch and parse performance schedules
                    logger.info(f"Processing schedules from: {schedule_url}")
                    self.process_performance_schedules(schedule_url, live_house)
                else:
                    logger.warning("No schedule URL found")

                # Update last collected datetime and state for the live house
                live_house.last_collected_datetime = timezone.now()
                live_house.last_collection_state = CrawlerCollectionState.SUCCESS
                live_house.save()

                # Update state to completed
                self.website.state = WebsiteProcessingState.COMPLETED
                self.website.save()
                logger.info(f"Successfully completed crawling: {self.website.url}")

            except requests.Timeout:
                # Handle timeout specifically
                logger.exception(f"Timeout while crawling {self.website.url}")
                if "live_house" in locals():
                    live_house.last_collection_state = CrawlerCollectionState.TIMEOUT
                    live_house.save()
                self.website.state = WebsiteProcessingState.FAILED
                self.website.save()
                raise
            except Exception:  # noqa: BLE001
                # Handle all other errors
                logger.exception(f"Error while crawling {self.website.url}")
                if "live_house" in locals():
                    live_house.last_collection_state = CrawlerCollectionState.ERROR
                    live_house.save()
                self.website.state = WebsiteProcessingState.FAILED
                self.website.save()
                raise

    def fetch_page(self, url: str) -> str:
        """Fetch the content of a page from the given URL."""
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()
        return response.text

    def fetch_page_js(
        self,
        url: str,
        wait_for_selector: str | None = None,
        click_load_more: bool = False,
        load_more_selector: str = "button:has-text('Load More'), button:has-text('„ÇÇ„Å£„Å®Ë¶ã„Çã')",
        max_clicks: int = 10,
    ) -> str:
        """
        Fetch page content using Playwright for JavaScript-rendered content.

        Args:
            url: URL to fetch
            wait_for_selector: CSS selector to wait for before capturing (optional)
            click_load_more: Whether to click "Load More" buttons until they disappear
            load_more_selector: CSS selector for "Load More" button
            max_clicks: Maximum number of times to click "Load More" (safety limit)

        Returns:
            Rendered HTML content after JavaScript execution
        """
        from playwright.sync_api import sync_playwright  # noqa: PLC0415

        logger.debug(f"Fetching page with Playwright: {url}")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()

            try:
                page.goto(url, timeout=self.timeout * 1000)

                # Wait for specific selector if provided
                if wait_for_selector:
                    logger.debug(f"Waiting for selector: {wait_for_selector}")
                    page.wait_for_selector(wait_for_selector, timeout=self.timeout * 1000)

                # Click "Load More" button repeatedly if requested
                if click_load_more:
                    clicks = 0
                    while clicks < max_clicks:
                        try:
                            # Check if load more button is visible
                            if page.is_visible(load_more_selector, timeout=2000):
                                logger.debug(f"Clicking load more button (click {clicks + 1})")
                                page.click(load_more_selector)
                                # Wait for new content to load
                                page.wait_for_timeout(1500)
                                clicks += 1
                            else:
                                logger.debug("Load more button not found, stopping")
                                break
                        except Exception:  # noqa: BLE001
                            logger.debug("Load more button no longer available")
                            break

                    if clicks > 0:
                        logger.debug(f"Clicked load more button {clicks} times")

                return page.content()
            finally:
                context.close()
                browser.close()

    def create_soup(self, html_content: str) -> BeautifulSoup:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """Create a BeautifulSoup object from HTML content."""
        return BeautifulSoup(html_content, "html.parser")

    def extract_live_house_info(self, html_content: str) -> dict[str, str]:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Extract live house information from the main page.
        Must return a dict with keys: name, name_kana, name_romaji, address, phone_number, capacity, opened_date

        Default implementation uses existing LiveHouse data and tries to extract additional info.
        Subclasses can override for site-specific logic.
        """
        return self._generic_extract_live_house_info(html_content)

    def find_schedule_link(self, html_content: str) -> str | None:
        """
        Find the link to the schedule page.

        Default implementation looks for common schedule-related keywords.
        Subclasses can override for site-specific logic.
        """
        return self._generic_find_schedule_link(html_content)

    def extract_performance_schedules(self, html_content: str) -> list[dict]:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Extract performance schedule information from schedule page.
        Must return a list of dicts with keys: date, open_time, start_time, performers

        Default implementation uses generic date/time pattern matching.
        Subclasses can override for site-specific logic.
        """
        return self._generic_extract_performance_schedules(html_content)

    def extract_ticket_info(self, html_content: str, context: str = "") -> PerformanceScheduleTicketPurchaseInfo | None:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Extract ticket information from schedule page or event context.
        Returns a PerformanceScheduleTicketPurchaseInfo object (not saved to DB yet).

        Default implementation uses generic pattern matching.
        Subclasses can override for site-specific logic.
        """
        return self._extract_ticket_info(html_content, context)

    def create_or_update_live_house(self, data: dict[str, str]) -> LiveHouse:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """Create or update a LiveHouse instance."""
        # Convert string date to date object if needed
        opened_date = data.get("opened_date")
        if isinstance(opened_date, str) and opened_date:
            opened_date = datetime.strptime(opened_date, "%Y-%m-%d").date()  # noqa: DTZ007
        elif not opened_date:
            # Default to a reasonable date if not found
            opened_date = date(2000, 1, 1)

        live_house, created = LiveHouse.objects.update_or_create(
            website=self.website,
            defaults={
                "name": data["name"],
                "name_kana": data.get("name_kana", ""),
                "name_romaji": data.get("name_romaji", ""),
                "address": data.get("address", ""),
                "phone_number": data.get("phone_number", ""),
                "capacity": int(data.get("capacity", 0)),
                "opened_date": opened_date,
            },
        )
        return live_house

    def process_performance_schedules(self, schedule_url: str, live_house: LiveHouse) -> None:
        """Process performance schedules including page traversal for multiple months."""
        logger.debug(f"Fetching schedule page: {schedule_url}")
        # Get current and next month pages
        current_month_content = self.fetch_page(schedule_url)
        logger.debug(f"Fetched {len(current_month_content)} characters from schedule page")

        schedules = self.extract_performance_schedules(current_month_content)
        logger.info(f"Extracted {len(schedules)} schedules from current month")

        # Try to find next month link
        next_month_url = self.find_next_month_link(current_month_content)
        logger.debug(f"Next month URL: {next_month_url}")

        if next_month_url:
            logger.debug(f"Fetching next month page: {next_month_url}")
            next_month_content = self.fetch_page(next_month_url)
            next_month_schedules = self.extract_performance_schedules(next_month_content)
            logger.info(f"Extracted {len(next_month_schedules)} schedules from next month")
            schedules.extend(next_month_schedules)

        logger.info(f"Creating {len(schedules)} performance schedule records")
        # Create PerformanceSchedule instances
        created_count = 0
        total_performers = set()

        for schedule_data in schedules:
            try:
                performance = self.create_performance_schedule(live_house, schedule_data)
                created_count += 1

                # Count performers for this schedule
                schedule_performers = performance.performers.all()
                for performer in schedule_performers:
                    total_performers.add(performer.name)

                perf_names = ", ".join([p.name for p in schedule_performers])
                logger.debug(f"Created schedule: {performance.performance_date} - {perf_names}")
            except Exception:  # noqa: BLE001
                logger.exception(f"Failed to create schedule for {schedule_data}")

        logger.info(f"üìä COLLECTION SUMMARY for {live_house.name}:")
        logger.info(f"  ‚úÖ Performance Schedules: {created_count} created")
        logger.info(f"  üé≠ Unique Performers: {len(total_performers)} discovered")
        logger.info(f"  üé™ Venue: {live_house.name} ({live_house.capacity} capacity)")

        if total_performers:
            performer_list = list(total_performers)[:5]  # Show first 5
            performers_text = ", ".join(performer_list)
            if len(total_performers) > 5:  # noqa: PLR2004
                performers_text += f" + {len(total_performers) - 5} more"
            logger.info(f"  üéµ Featured Artists: {performers_text}")

        logger.info(f"Successfully created {created_count} performance schedules")

    def find_next_month_link(self, html_content: str) -> str | None:
        """
        Find the link to next month's schedule page.

        Default implementation looks for common next month keywords.
        Subclasses can override for site-specific logic.
        """
        return self._generic_find_next_month_link(html_content)

    def create_performance_schedule(self, live_house: LiveHouse, data: dict) -> PerformanceSchedule:  # noqa: C901, PLR0912, PLR0915
        """Create a PerformanceSchedule instance with ticket information."""
        # Convert string date/time to appropriate objects
        performance_date = data["date"]
        if isinstance(performance_date, str):
            performance_date = datetime.strptime(performance_date, "%Y-%m-%d").date()  # noqa: DTZ007

        open_time = data["open_time"]
        if isinstance(open_time, str):
            open_time = datetime.strptime(open_time, "%H:%M").time()  # noqa: DTZ007

        start_time = data["start_time"]
        if isinstance(start_time, str):
            start_time = datetime.strptime(start_time, "%H:%M").time()  # noqa: DTZ007

        # Process and validate performers BEFORE any database operations
        performer_names = data.get("performers", [])
        if isinstance(performer_names, str):
            # Split by common delimiters
            performer_names = re.split(r"[,„ÄÅ/]", performer_names)
            performer_names = [name.strip() for name in performer_names if name.strip()]

        # Filter and clean performer names
        clean_performer_names = []
        for name in performer_names:
            cleaned_name = self._clean_performer_name(name)
            if cleaned_name and self._is_valid_performer_name(cleaned_name):
                clean_performer_names.append(cleaned_name)

        # Validate all performers and collect valid ones BEFORE database operations
        valid_performers = []
        for performer_name in clean_performer_names:
            # Check if performer already exists
            existing_performer = Performer.objects.filter(name=performer_name).first()
            if existing_performer:
                # Existing performer, assume it's already validated
                valid_performers.append(existing_performer)
                logger.debug(f"‚úÖ Using existing validated performer: {performer_name}")
            else:
                # New performer - validate BEFORE creating in DB
                logger.debug(f"üîç Validating new performer: {performer_name}")

                # Convert name to Kana and Romaji using pykakasi
                kks = kakasi()
                result = kks.convert(performer_name)
                performer_name_kana = "".join([item["kana"] for item in result])
                performer_name_romaji = "".join([item["hepburn"] for item in result])
                performer = Performer(
                    name=performer_name,
                    name_kana=performer_name_kana,
                    name_romaji=performer_name_romaji,
                )

                try:
                    # Validate BEFORE saving to database
                    self._search_for_performer_details(performer)
                    # If validation succeeds, add to valid list (will save later)
                    valid_performers.append(performer)
                    logger.info(f"‚úÖ Validated new performer: {performer_name}")

                except PerformerValidationError as e:
                    # Log error for failed validation with venue and date context
                    logger.exception(
                        f"‚ùå Skipping performer '{performer_name}' for {live_house.name} on {performance_date}: {e}"  # noqa: TRY401
                    )
                except Exception as e:  # noqa: BLE001
                    logger.exception(
                        f"‚ùå Failed to process performer '{performer_name}' for {live_house.name} "
                        f"on {performance_date}: {e}"  # noqa: TRY401
                    )

        # Skip schedule if no valid performers
        if not valid_performers:
            logger.warning(
                f"‚ö†Ô∏è Skipping schedule for {live_house.name} on {performance_date}: No valid performers found"
            )
            raise ValueError("No valid performers for this schedule")  # noqa: B904

        # NOW create the performance schedule (all validation passed)
        defaults = {
            "open_time": open_time,
        }

        # Add performance name if provided
        if "performance_name" in data and data["performance_name"]:
            defaults["performance_name"] = data["performance_name"]

        performance, created = PerformanceSchedule.objects.get_or_create(
            live_house=live_house, performance_date=performance_date, start_time=start_time, defaults=defaults
        )

        # Save new performers and add all valid performers to the schedule
        for performer in valid_performers:
            if performer.pk is None:  # New performer not yet saved
                # Use get_or_create to avoid UNIQUE constraint failures on name_romaji
                # If a performer with the same romaji exists, use that one instead
                defaults = {
                    "name": performer.name,
                    "name_kana": performer.name_kana,
                }
                # Add website if it's set on the performer object
                if hasattr(performer, "website") and performer.website:
                    defaults["website"] = performer.website

                existing_performer, created = Performer.objects.get_or_create(
                    name_romaji=performer.name_romaji, defaults=defaults
                )
                if created:
                    logger.info(f"‚úÖ Created performer in database: {existing_performer.name}")
                else:
                    logger.info(
                        f"‚ÑπÔ∏è Using existing performer with same romaji: {existing_performer.name} "
                        f"(requested: {performer.name}, romaji: {performer.name_romaji})"
                    )
                # Use the existing performer for the schedule
                performer = existing_performer  # noqa: PLW2901
            performance.performers.add(performer)

        # Extract and create/update ticket information from the context
        if "context" in data:
            ticket_info = self.extract_ticket_info("", data["context"])
            if ticket_info:
                self._create_or_update_ticket_info(performance, ticket_info)

        return performance

    # Generic helper methods that concrete implementations can use
    def _generic_extract_live_house_info(self, html_content: str) -> dict[str, str]:  # noqa: C901, PLR0912, PLR0915
        """
        Generic helper method to extract live house information.
        Uses existing LiveHouse instance and tries to extract additional info from the page.
        """
        soup = self.create_soup(html_content)

        # Get existing LiveHouse instance
        existing_livehouse = self.website.live_houses.first()
        if not existing_livehouse:
            raise ValueError(f"No LiveHouse entry found for website {self.website.url}")  # noqa: B904

        # Try to extract additional information from the page
        page_text = soup.get_text()

        # Try to find address if not set
        if not existing_livehouse.address:
            address_patterns = [
                r"[„Äí]\s*(\d{3}-\d{4})\s*([^0-9\n]{10,50})",
                r"‰ΩèÊâÄ[Ôºö:\s]*([^0-9\n]{10,50})",
                r"Address[Ôºö:\s]*([^0-9\n]{10,50})",
                r"Êù±‰∫¨ÈÉΩ[^0-9\n]{5,40}",
            ]

            for pattern in address_patterns:
                match = re.search(pattern, page_text)
                if match:
                    if len(match.groups()) > 1:
                        existing_livehouse.address = match.group(2).strip()
                    else:
                        existing_livehouse.address = match.group(1).strip()
                    break

        # Try to find phone number if not set
        if not existing_livehouse.phone_number:
            phone_patterns = [
                r"(?:ÈõªË©±|TEL|Phone)[Ôºö:\s]*(\d{2,4}[-‚Äê]\d{3,4}[-‚Äê]\d{3,4})",
                r"(\d{2,4}[-‚Äê]\d{3,4}[-‚Äê]\d{3,4})",
            ]

            for pattern in phone_patterns:
                match = re.search(pattern, page_text)
                if match:
                    existing_livehouse.phone_number = match.group(1).strip()
                    break

        # Try to find capacity if not set
        if existing_livehouse.capacity == 0:
            capacity_patterns = [
                r"(?:„Ç≠„É£„Éë|„Ç≠„É£„Éë„Ç∑„ÉÜ„Ç£|ÂèéÂÆπ)[Ôºö:\s]*(\d+)",
                r"(?:capacity|Capacity)[Ôºö:\s]*(\d+)",
                r"(\d+)\s*(?:‰∫∫|Âêç|persons?)",
            ]

            for pattern in capacity_patterns:
                match = re.search(pattern, page_text)
                if match:
                    capacity = int(match.group(1))
                    if 50 <= capacity <= 2000:  # Reasonable range for live houses  # noqa: PLR2004
                        existing_livehouse.capacity = capacity
                        break

        # Return the data in the expected format for create_or_update_live_house
        return {
            "name": existing_livehouse.name,
            "name_kana": existing_livehouse.name_kana,
            "name_romaji": existing_livehouse.name_romaji,
            "address": existing_livehouse.address,
            "phone_number": existing_livehouse.phone_number,
            "capacity": str(existing_livehouse.capacity),
            "opened_date": existing_livehouse.opened_date.strftime("%Y-%m-%d"),
        }

    def _generic_find_schedule_link(self, html_content: str) -> str | None:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """Generic helper method to find schedule page link."""
        soup = self.create_soup(html_content)

        # Look for schedule-related links
        schedule_keywords = [
            "schedule",
            "„Çπ„Ç±„Ç∏„É•„Éº„É´",
            "live",
            "„É©„Ç§„Éñ",
            "event",
            "„Ç§„Éô„É≥„Éà",
            "calendar",
            "„Ç´„É¨„É≥„ÉÄ„Éº",
        ]

        links = soup.find_all("a", href=True)
        for link in links:
            href = link.get("href")
            text = link.get_text().lower()

            for keyword in schedule_keywords:
                if keyword in text or keyword in href.lower():
                    return urljoin(self.base_url, href)

        # Also check for navigation menus
        nav_elements = soup.find_all(["nav", "ul", "div"], class_=re.compile(r"(menu|nav|header)", re.IGNORECASE))
        for nav in nav_elements:
            nav_links = nav.find_all("a", href=True)
            for link in nav_links:
                href = link.get("href")
                text = link.get_text().lower()

                for keyword in schedule_keywords:
                    if keyword in text:
                        return urljoin(self.base_url, href)

        return None

    def _generic_extract_performance_schedules(self, html_content: str) -> list[dict]:  # noqa: C901, PLR0912, PLR0915
        """Generic helper method to extract performance schedules."""
        schedules = []
        soup = self.create_soup(html_content)

        # Look for date patterns in the content
        text = soup.get_text()

        # Common date patterns for Japanese websites
        date_patterns = [
            r"(\d{4})[Âπ¥/-](\d{1,2})[Êúà/-](\d{1,2})[Êó•]?",
            r"(\d{1,2})[Êúà/-](\d{1,2})[Êó•]?",
            r"(\d{4})-(\d{1,2})-(\d{1,2})",
        ]

        current_year = 2025  # Default year
        found_dates = set()

        for pattern in date_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                groups = match.groups()

                if len(groups) == 3:  # Year, month, day  # noqa: PLR2004
                    year, month, day = groups
                elif len(groups) == 2:  # Month, day (use current year)  # noqa: PLR2004
                    month, day = groups
                    year = str(current_year)
                else:
                    continue

                try:
                    year_int = int(year)
                    month_int = int(month)
                    day_int = int(day)

                    # Validate date ranges
                    if 2024 <= year_int <= 2026 and 1 <= month_int <= 12 and 1 <= day_int <= 31:  # noqa: PLR2004
                        date_str = f"{year_int}-{month_int:02d}-{day_int:02d}"
                        if date_str not in found_dates:
                            found_dates.add(date_str)

                            # Try to find time information near this date
                            context_start = max(0, match.start() - 200)
                            context_end = min(len(text), match.end() + 200)
                            context = text[context_start:context_end]

                            # Look for time patterns
                            time_match = re.search(r"(\d{1,2}):(\d{2})", context)
                            if time_match:
                                start_time = f"{time_match.group(1).zfill(2)}:{time_match.group(2)}"
                            else:
                                start_time = "19:00"  # Default

                            # Extract performer/event info from context
                            performers = []
                            # Look for artist names or event titles in the context
                            lines = context.split("\n")
                            for raw_line in lines:
                                cleaned_line = raw_line.strip()
                                if (
                                    cleaned_line
                                    and not re.match(r"^[\d\s/:.-]+$", cleaned_line)
                                    and len(cleaned_line) > 2  # noqa: PLR2004
                                ):  # noqa: PLR2004, E501
                                    # This might be a performer or event name
                                    performers.append(cleaned_line[:50])  # Limit length
                                    if len(performers) >= 3:  # noqa: PLR2004  # Limit to 3 performers
                                        break

                            if not performers:
                                performers = ["Live Event"]

                            schedules.append(
                                {
                                    "date": date_str,
                                    "open_time": "18:30",  # Default
                                    "start_time": start_time,
                                    "performers": performers[:3],  # Max 3 performers
                                }
                            )

                            # Limit to reasonable number of schedules
                            if len(schedules) >= 20:  # noqa: PLR2004
                                break

                except (ValueError, IndexError):
                    continue

        return schedules[:20]  # Return max 20 schedules

    def _generic_find_next_month_link(self, html_content: str) -> str | None:  # noqa: C901, PLR0912, PLR0915
        """
        Generic helper method to find next month link.

        Uses multiple strategies to find next month navigation:
        - Text and href pattern matching
        - Year-aware date-based patterns
        - Navigation area targeting
        """
        soup = self.create_soup(html_content)

        # Calculate next month with year-aware logic
        current_date = timezone.localdate()
        next_month = (current_date.month % 12) + 1
        next_year = current_date.year if next_month > current_date.month else current_date.year + 1

        # Expanded navigation patterns
        next_patterns = [
            "next",
            "Ê¨°",
            "ÁøåÊúà",
            "Êù•Êúà",
            "‚Üí",
            "Ôºû",
            ">",
            ">>",
            "‚Ä∫",
            "Ê¨°„Å∏",
            "Ê¨°Êúà",
            "next month",
            "forward",
        ]

        # Check all links
        links = soup.find_all("a", href=True)
        for link in links:
            href = link.get("href", "")
            text = link.get_text(strip=True).lower()

            # Skip javascript and anchors
            if not href or href.startswith(("#", "javascript:")):
                continue

            # Check text for navigation patterns
            for pattern in next_patterns:
                if pattern.lower() in text:
                    full_url = urljoin(self.base_url, href)
                    logger.debug(f"Found potential next month link by text pattern '{pattern}': {full_url}")
                    return full_url

            # Check href for navigation patterns
            href_lower = href.lower()
            for pattern in next_patterns:
                if pattern in href_lower:
                    full_url = urljoin(self.base_url, href)
                    logger.debug(f"Found potential next month link in href: {full_url}")
                    return full_url

        # Look for month-based navigation with year awareness
        month_patterns = [
            f"{next_year}/{next_month:02d}",
            f"{next_year}-{next_month:02d}",
            f"{next_year}Âπ¥{next_month}Êúà",
            f"{next_month}Êúà",
            f"{next_month:02d}",
        ]

        for pattern in month_patterns:
            for link in links:
                text = link.get_text(strip=True)
                href = link.get("href", "")

                # Skip javascript and anchors
                if not href or href.startswith(("#", "javascript:")):
                    continue

                # Check in text or href
                if pattern in text or pattern in href:
                    full_url = urljoin(self.base_url, href)
                    logger.debug(f"Found next month link by date pattern '{pattern}': {full_url}")
                    return full_url

        # Look in navigation areas
        nav_areas = soup.find_all(["nav", "div", "ul"], class_=re.compile(r"pag|nav|calendar|month", re.IGNORECASE))
        for nav in nav_areas:
            nav_links = nav.find_all("a", href=True)
            for link in nav_links:
                href = link.get("href", "")
                text = link.get_text(strip=True).lower()

                if not href or href.startswith(("#", "javascript:")):
                    continue

                if any(p.lower() in text for p in next_patterns):
                    full_url = urljoin(self.base_url, href)
                    logger.debug(f"Found next month link in navigation area: {full_url}")
                    return full_url

        logger.debug("No next month link found")
        return None

    def _clean_performer_name(self, name: str) -> str:
        """Clean and normalize performer name for Japanese performers."""
        if not name:
            return ""

        # Remove common prefixes/suffixes that aren't part of performer names
        name = name.strip()

        # Remove pricing information
        name = re.sub(r"[¬•Ôø•]\s*\d+[,\d]*", "", name)

        # Remove time information
        name = re.sub(r"\d{1,2}:\d{2}", "", name)

        # Remove drink charge information
        name = re.sub(r"\(.*1D.*\)", "", name)
        name = re.sub(r"\+1D", "", name)
        name = re.sub(r"ÂÖ•Â†¥ÊôÇÂà•ÈÄî1D", "", name)

        # Remove day of week indicators
        name = re.sub(r"[Ôºà(][ÊúàÁÅ´Ê∞¥Êú®ÈáëÂúüÊó•][Ôºâ)]", "", name)

        # Remove excessive whitespace and clean up
        name = re.sub(r"\s+", " ", name)
        name = name.strip("- /\\()[]{}„ÄÅ„ÄÇ")

        return name.strip()

    def _is_valid_performer_name(self, name: str) -> bool:
        """Check if a name is likely to be a valid performer name (Japanese-focused)."""
        if not name or len(name) < 2:  # noqa: PLR2004
            return False

        # Filter out obvious non-performer content
        invalid_patterns = [
            r"^[¬•Ôø•]\d+",  # Price only
            r"^\d{1,2}:\d{2}",  # Time only
            r"^(ABOUT|HOME|SCHEDULE|ACCESS|NEWS|CONTACT|TICKET|STAFF|ACT|LIVE)$",  # Navigation (English)
            r"^\d{4}[-/Âπ¥]\d{1,2}[-/Êúà]\d{1,2}[Êó•]?$",  # Date only
            r"^(Êúà|ÁÅ´|Ê∞¥|Êú®|Èáë|Âúü|Êó•)$",  # Day of week only
            r"^\d+$",  # Pure numbers
            r"^[-/\\()ÔºàÔºâ]+$",  # Pure punctuation
            r"^,\d+$",  # Price fragment
            r"FOOD[:Ôºö]",  # Food info
            r"ÂÖ•Â†¥ÊôÇÂà•ÈÄî",  # Drink charge info
            r"start|open|door",  # Event timing (English)
            r"^(‰∫àÁ¥Ñ|ÊñôÈáë|ÊôÇÈñì|ÈñãÂ†¥|ÈñãÊºî)$",  # Event info (Japanese)
        ]

        for pattern in invalid_patterns:
            if re.search(pattern, name, re.IGNORECASE):
                return False

        # Must contain meaningful characters (Japanese or English letters)
        if not re.search(r"[a-zA-Z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]", name):
            return False

        # Too short single characters are likely not performer names
        return len(name) != 1  # noqa: PLR2004

    def _search_for_performer_details(self, performer: Performer) -> None:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Search for additional performer details, update name formatting, and discover social media links.
        Validates performer by searching for their online presence (social media or dedicated website).
        Raises PerformerValidationError if the performer fails validation.
        """
        try:
            logger.debug(f"Starting validation search for performer: '{performer.name}'")

            # Step 1: Handle Japanese name formatting
            self._format_japanese_performer_name(performer)

            # Step 2: Search for band details online (including website discovery)
            band_info = self._search_band_details(performer.name)
            if band_info:
                self._update_performer_from_band_info(performer, band_info)
                logger.debug(f"Found band website for {performer.name}: {band_info.get('website', 'N/A')}")

            # Step 3: Search for social media links
            social_links = self._search_social_media_links(performer.name)
            if social_links:
                self._update_performer_social_links(performer, social_links)
                logger.debug(f"Found {len(social_links)} social links for {performer.name}")

            # NOTE: Do NOT save performer here - validation should not do database operations
            # The performer will be saved later in create_performance_schedule if validation passes

            # Step 4: Validate that we found legitimate online presence
            # This is the critical validation step - only validate based on actual online presence
            try:
                performer.validate_full_artist_profile()
                logger.info(f"‚úÖ Validated artist profile: {performer.name} (found valid online presence)")
            except ValidationError:
                # If we can't find any online presence, this is likely not a real performer
                logger.warning(f"‚ùå Rejecting '{performer.name}': No valid online presence found")
                raise PerformerValidationError(  # noqa: B904
                    f"Cannot validate '{performer.name}' as a legitimate performer: "
                    f"No social media accounts or dedicated artist website found. "
                    f"This may be venue information, staff, or non-performer content."
                )

            logger.debug(f"Successfully validated performer: {performer.name}")

        except PerformerValidationError as e:
            # Re-raise our custom validation error
            raise PerformerValidationError from e
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Failed to search for performer details for {performer.name}: {str(e)}")

    def _format_japanese_performer_name(self, performer: Performer) -> None:
        """Format Japanese performer names by parsing different notation patterns."""
        name = performer.name

        # Handle Japanese parenthetical notation: „Éê„É≥„ÉâÂêçÔºàË™≠„ÅøÊñπÔºâ
        if "Ôºà" in name and "Ôºâ" in name:
            match = re.search(r"([^Ôºà]+)Ôºà([^Ôºâ]+)Ôºâ", name)
            if match:
                main_name = match.group(1).strip()
                reading = match.group(2).strip()

                # Main name is usually the band/artist name
                performer.name = main_name
                # Reading could be kana or romaji
                if re.search(r"[\u3040-\u309F\u30A0-\u30FF]", reading):
                    performer.name_kana = reading
                else:
                    performer.name_romaji = reading

        # Handle slash notation: Êó•Êú¨Ë™ûÂêç/English Name
        elif "/" in name and len(name.split("/")) == 2:  # noqa: PLR2004
            parts = [p.strip() for p in name.split("/")]
            part1, part2 = parts

            # Determine which part is Japanese vs English/Romaji
            has_jp1 = re.search(r"[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]", part1)
            has_jp2 = re.search(r"[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]", part2)

            if has_jp1 and not has_jp2:
                # Part1 is Japanese, Part2 is English/Romaji
                performer.name = part1
                performer.name_romaji = part2
            elif has_jp2 and not has_jp1:
                # Part1 is English/Romaji, Part2 is Japanese
                performer.name = part2
                performer.name_romaji = part1

        # Handle middle dot notation: „Ç´„Çø„Ç´„Éä„ÉªÂêçÂâç
        elif "„Éª" in name:
            # This is often used in katakana names, keep as-is but might be kana
            if re.search(r"[\u30A0-\u30FF]", name):  # Contains katakana
                performer.name_kana = name

    def _search_band_details(self, band_name: str) -> dict | None:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Search for band details using web search.
        Returns basic information about the band/artist.
        """
        try:
            # Create search queries for Japanese bands
            search_queries = [
                f"{band_name} „Éê„É≥„Éâ",
                f"{band_name} „Ç¢„Éº„ÉÜ„Ç£„Çπ„Éà",
                f"{band_name} Èü≥Ê•Ω",
                f"{band_name} band music",
            ]

            for query in search_queries:
                try:
                    # Simple web search to find band information
                    search_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"

                    # Set user agent to avoid being blocked
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"  # noqa: E501
                    }

                    response = self.session.get(search_url, headers=headers, timeout=10)
                    if response.status_code == 200:  # noqa: PLR2004
                        soup = BeautifulSoup(response.text, "html.parser")  # noqa: F841

                        # Extract basic information from search results
                        band_info = {}

                        # Look for website links in search results
                        website_patterns = [
                            r'(https?://[^/]*\.(?:com|net|org|jp|co\.jp)[^"\s]*)',
                            r'(https?://[^/]*(?:bandcamp|soundcloud|spotify)\.com[^"\s]*)',
                        ]

                        for pattern in website_patterns:
                            matches = re.findall(pattern, response.text)
                            for match in matches:
                                if any(
                                    platform in match.lower() for platform in ["official", "band", "music", "artist"]
                                ):
                                    band_info["website"] = match
                                    break
                            if "website" in band_info:
                                break

                        if band_info:
                            logger.debug(f"Found band info for {band_name}: {band_info}")
                            return band_info

                except Exception as e:  # noqa: BLE001
                    logger.debug(f"Search failed for query '{query}': {str(e)}")
                    continue

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Band search failed for {band_name}: {str(e)}")

        return None

    def _search_social_media_links(self, band_name: str) -> list[dict]:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """
        Search for social media links for the band/artist.
        Returns a list of social media platform information.
        """
        social_links = []

        try:
            # Search for common social media platforms
            platforms = {
                "twitter": ["twitter.com", "x.com"],
                "instagram": ["instagram.com"],
                "youtube": ["youtube.com", "youtu.be"],
                "facebook": ["facebook.com"],
                "bandcamp": ["bandcamp.com"],
                "soundcloud": ["soundcloud.com"],
                "spotify": ["spotify.com"],
                "apple_music": ["music.apple.com"],
                "tiktok": ["tiktok.com"],
                "discord": ["discord.gg", "discord.com"],
                "twitch": ["twitch.tv"],
                "reddit": ["reddit.com"],
                "linkedin": ["linkedin.com"],
                "vimeo": ["vimeo.com"],
                "github": ["github.com"],
                "patreon": ["patreon.com"],
                "mastodon": ["mastodon.social", "mastodon.online"],
            }

            search_query = f"{band_name} social media twitter instagram youtube"
            search_url = f"https://www.google.com/search?q={search_query.replace(' ', '+')}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"  # noqa: E501
            }

            response = self.session.get(search_url, headers=headers, timeout=10)
            if response.status_code == 200:  # noqa: PLR2004
                text = response.text

                # Extract social media URLs
                url_pattern = r'https?://[^"\s<>]+'
                urls = re.findall(url_pattern, text)

                for url in urls:
                    for platform, domains in platforms.items():
                        for domain in domains:
                            if domain in url.lower():
                                # Extract platform ID from URL
                                platform_id = self._extract_platform_id(url, platform)
                                if platform_id:
                                    social_links.append({"platform": platform, "platform_id": platform_id, "url": url})
                                break

                        # Limit to avoid too many duplicate results
                        if len(social_links) >= 5:  # noqa: PLR2004
                            break

                    if len(social_links) >= 5:  # noqa: PLR2004
                        break

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Social media search failed for {band_name}: {str(e)}")

        return social_links[:5]  # Limit to 5 social links

    def _extract_platform_id(self, url: str, platform: str) -> str | None:  # noqa: C901, PLR0912, PLR0915, PLR0911
        """Extract platform-specific ID from social media URL."""
        try:
            if platform == "twitter":
                match = re.search(r"(?:twitter\.com|x\.com)/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "instagram":
                match = re.search(r"instagram\.com/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "youtube":
                match = re.search(r"youtube\.com/(?:c/|channel/|user/)?([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "facebook":
                match = re.search(r"facebook\.com/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform in ["bandcamp", "soundcloud", "spotify"]:
                match = re.search(rf"{platform}\.com/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "tiktok":
                match = re.search(r"tiktok\.com/@?([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "discord":
                match = re.search(r"discord\.(?:gg|com)/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "twitch":
                match = re.search(r"twitch\.tv/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "reddit":
                match = re.search(r"reddit\.com/(?:r/|u/|user/)?([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform in ["linkedin", "vimeo", "github", "patreon"]:
                match = re.search(rf"{platform}\.com/([^/?\s]+)", url)
                return match.group(1) if match else None
            if platform == "mastodon":
                match = re.search(r"mastodon\.(?:social|online)/(@?[^/?\s]+)", url)
                return match.group(1) if match else None

        except Exception:  # noqa: BLE001, S110
            pass

        return None

    def _update_performer_from_band_info(self, performer: Performer, band_info: dict) -> None:
        """Update performer with information found from band search."""
        try:
            if "website" in band_info and not performer.website:
                performer.website = band_info["website"]
                logger.debug(f"Added website for {performer.name}: {band_info['website']}")

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to update performer from band info: {str(e)}")

    def _update_performer_social_links(self, performer: Performer, social_links: list[dict]) -> None:
        """Update performer with discovered social media links."""
        try:
            for link_info in social_links:
                # Check if this social link already exists
                existing_link = PerformerSocialLink.objects.filter(
                    performer=performer, platform=link_info["platform"], platform_id=link_info["platform_id"]
                ).first()

                if not existing_link:
                    PerformerSocialLink.objects.create(
                        performer=performer,
                        platform=link_info["platform"],
                        platform_id=link_info["platform_id"],
                        url=link_info["url"],
                    )
                    logger.debug(f"Added {link_info['platform']} link for {performer.name}: {link_info['url']}")

        except Exception as e:  # noqa: BLE001
            logger.debug(f"Failed to update social links: {str(e)}")

    def _extract_ticket_info(  # noqa: C901, PLR0912, PLR0915, PLR0911
        self, html_content: str, context: str = ""
    ) -> PerformanceScheduleTicketPurchaseInfo | None:
        """
        Generic helper method to extract ticket information from HTML content.
        Returns a PerformanceScheduleTicketPurchaseInfo object (not saved to DB yet).
        """
        soup = self.create_soup(html_content)
        text = soup.get_text() + " " + context

        # Initialize a ticket info object (not saved to DB yet)
        ticket_info = PerformanceScheduleTicketPurchaseInfo()
        has_ticket_data = False

        # Extract email addresses
        email_patterns = [
            r"(?:„ÉÅ„Ç±„ÉÉ„Éà|ticket|‰∫àÁ¥Ñ|reservation)[Ôºö:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})",
            r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})",
        ]

        for pattern in email_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                email = match.group(1) if len(match.groups()) > 0 else match.group(0)
                # Filter out common non-ticket emails
                if not any(domain in email.lower() for domain in ["facebook", "twitter", "instagram", "youtube"]):
                    ticket_info.ticket_contact_email = email
                    has_ticket_data = True
                    break

        # Extract phone numbers
        phone_patterns = [
            r"(?:„ÉÅ„Ç±„ÉÉ„Éà|ticket|‰∫àÁ¥Ñ|reservation)[Ôºö:\s]*(\d{2,4}[-‚Äê]\d{3,4}[-‚Äê]\d{3,4})",
            r"(?:ÈÄ£Áµ°|contact|ÂïèÂêà|„ÅäÂïè„ÅÑÂêà„Çè„Åõ)[Ôºö:\s]*(\d{2,4}[-‚Äê]\d{3,4}[-‚Äê]\d{3,4})",
        ]

        for pattern in phone_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                ticket_info.ticket_contact_phone = match.group(1)
                has_ticket_data = True
                break

        # Extract ticket URLs
        url_patterns = [
            r"(?:„ÉÅ„Ç±„ÉÉ„Éà|ticket|‰∫àÁ¥Ñ|reservation)[Ôºö:\s]*(?:URL[Ôºö:\s]*)?(https?://[^\s\)]+)",
            r"(https?://[^\s]*(?:ticket|peatix|eventbrite|eplus|cnplayguide)[^\s]*)",
        ]

        for pattern in url_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                url = match.group(1) if len(match.groups()) > 0 else match.group(0)
                # Clean up URL
                url = url.rstrip(".,;)")
                ticket_info.ticket_url = url
                has_ticket_data = True
                break

        # Extract ticket prices
        price_patterns = [
            r"(?:„ÉÅ„Ç±„ÉÉ„Éà|ticket|ÊñôÈáë|price)[Ôºö:\s]*[¬•Ôø•]?\s*(\d{1,2},?\d{3})[ÂÜÜ¬•]?",
            r"(?:ÂâçÂ£≤|advance)[Ôºö:\s]*[¬•Ôø•]?\s*(\d{1,2},?\d{3})[ÂÜÜ¬•]?",
            r"(?:ÂΩìÊó•|door)[Ôºö:\s]*[¬•Ôø•]?\s*(\d{1,2},?\d{3})[ÂÜÜ¬•]?",
        ]

        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                price_str = match.group(1).replace(",", "")
                try:
                    price = float(price_str)
                    # Reasonable price range for live house tickets
                    if 500 <= price <= 20000:  # noqa: PLR2004
                        ticket_info.ticket_price = price
                        has_ticket_data = True
                        break
                except ValueError:
                    continue

        # Extract sales dates
        date_patterns = [
            r"(?:Áô∫Â£≤|sale|Ë≤©Â£≤)[Ôºö:\s]*(\d{4})[Âπ¥/-](\d{1,2})[Êúà/-](\d{1,2})[Êó•]?",
            r"(?:Âèó‰ªò|reception)[Ôºö:\s]*(\d{4})[Âπ¥/-](\d{1,2})[Êúà/-](\d{1,2})[Êó•]?",
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    year = int(match.group(1))
                    month = int(match.group(2))
                    day = int(match.group(3))

                    if 2024 <= year <= 2026 and 1 <= month <= 12 and 1 <= day <= 31:  # noqa: PLR2004
                        date_obj = date(year, month, day)
                        ticket_info.ticket_sales_start_date = timezone.datetime.combine(
                            date_obj, timezone.datetime.min.time()
                        ).replace(tzinfo=timezone.get_current_timezone())
                        has_ticket_data = True
                        break
                except (ValueError, IndexError):
                    continue

        return ticket_info if has_ticket_data else None

    def _create_or_update_ticket_info(
        self, performance: PerformanceSchedule, ticket_info: PerformanceScheduleTicketPurchaseInfo
    ) -> None:
        """Create or update PerformanceScheduleTicketPurchaseInfo for a performance."""
        try:
            # Prepare data for update_or_create
            defaults = {}

            if ticket_info.ticket_contact_email:
                defaults["ticket_contact_email"] = ticket_info.ticket_contact_email
            if ticket_info.ticket_contact_phone:
                defaults["ticket_contact_phone"] = ticket_info.ticket_contact_phone
            if ticket_info.ticket_url:
                defaults["ticket_url"] = ticket_info.ticket_url
            if ticket_info.ticket_price:
                defaults["ticket_price"] = ticket_info.ticket_price
            if ticket_info.ticket_sales_start_date:
                defaults["ticket_sales_start_date"] = ticket_info.ticket_sales_start_date
            if ticket_info.ticket_sales_end_date:
                defaults["ticket_sales_end_date"] = ticket_info.ticket_sales_end_date

            if not defaults:
                logger.debug(f"No ticket info to save for performance {performance}")
                return

            # Create or update ticket purchase info
            saved_ticket_info, created = PerformanceScheduleTicketPurchaseInfo.objects.update_or_create(
                performance=performance, defaults=defaults
            )

            action = "Created" if created else "Updated"
            logger.debug(f"{action} ticket info for performance {performance}: {list(defaults.keys())}")

        except Exception as e:  # noqa: BLE001
            logger.warning(f"Failed to create/update ticket info for {performance}: {str(e)}")
